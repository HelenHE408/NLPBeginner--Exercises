{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c97054-101e-4961-b566-54ac976ec2f3",
   "metadata": {},
   "source": [
    "# lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b6dacd-2722-4248-b851-bdf4d9fcc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f478f499-f9fc-41ed-aa77-92fc06401e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69511423-a9e9-4163-b078-e53ab44f5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3bc232b-8d76-484a-b437-ec4358647ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520fc00-ad83-4e43-ae6f-806fd9ba6cec",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b0b6596-ab58-451f-8dad-04096d251721",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"news.txt\", \"r\")\n",
    "raw_text = []\n",
    "for line in file:\n",
    "    raw_text.append(line.strip())\n",
    "file.close()\n",
    "# 为了减少运算量少取几行\n",
    "raw_text = [raw_text[i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665fa4d-9a6f-40a6-9ed4-ae4b06d37f96",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4d49eb-9a6a-4108-be6f-74c45c8bf51c",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7179d360-d755-49a2-b933-196adde82570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dePunctuation(line):\n",
    "    line = line.translate(str.maketrans('', '', string.punctuation))\n",
    "    return line\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def remove_lowFrequency(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in low_freq_vocab]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def lowFrequency(text, threshold = 0):\n",
    "    # 处理corpus\n",
    "    tokens = [line.split() for line in text]\n",
    "    corpus = [token.lower() for sub_tokens in tokens for token in sub_tokens]\n",
    "    cha_count = collections.Counter(corpus)\n",
    "    cha_count_modified = {key: value / len(corpus) for key, value in cha_count.items()}\n",
    "    low_freq_vocab = [word for word, count in cha_count_modified.items() if count < threshold]\n",
    "    return low_freq_vocab\n",
    "\n",
    "def clean(text, type='word', stop = True):\n",
    "    cleaned_text = []\n",
    "    if type == 'word':\n",
    "        for line in text:\n",
    "            line = dePunctuation(line)\n",
    "            if stop:\n",
    "                line = remove_stopwords(line)\n",
    "            line = remove_lowFrequency(line)\n",
    "            cleaned_text.append(line)\n",
    "    elif type == 'char':\n",
    "        cleaned_text = [dePunctuation(line) for line in text]\n",
    "        cleaned_text = [token for line in cleaned_text for token in line]\n",
    "    cleaned_text = list(filter(lambda x: x != '', cleaned_text))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57b4e03d-d9a1-42fb-9d61-9a497859f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_center(clean_text, window_size=2):\n",
    "    flatten_text = ' '.join(clean_text).split()\n",
    "    data = []\n",
    "    for i, center in enumerate(flatten_text):\n",
    "        if (i-window_size>=0) and (i+window_size<len(flatten_text)):\n",
    "            context = [flatten_text[j] for j in range(i-window_size, i+window_size) \n",
    "                   if (j!=i)]\n",
    "            data.append((context, center))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8702088d-c16c-47e9-add4-4e4ad41c5709",
   "metadata": {},
   "source": [
    "### call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d0916d-b108-460c-9ecc-c5e940c3c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98107274-506a-46f8-afd8-6d95ae1d4135",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_freq_vocab = lowFrequency(raw_text, threshold = 0.0001)\n",
    "clean_text = clean(raw_text, type = 'word', stop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6a8b6f7-ff83-4121-b796-3c93c479dadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [line.split() for line in clean_text]\n",
    "corpus = [word for line in corpus for word in line]\n",
    "vocab = set(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b1b8839-0ae1-4e40-b645-ff99ce088589",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {word:ix for ix, word in enumerate(vocab)}\n",
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "data = context_center(clean_text, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9919046-2620-493b-8c9b-26d16790137b",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7d0d4-907f-411d-9db2-53362f46f1af",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "203c1958-f0bf-4eb5-83a1-ae821c41e37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_context_vector(context, word_to_ix):\n",
    "    idxs = [word_to_ix[w] for w in context]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a967dc-e7ff-443c-921f-ed25de266929",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self,vocab_size, hidden_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        # embedding层形状\n",
    "        self.v_embedding = nn.Embedding(vocab_size, embedding_dim) # as center\n",
    "        self.u_embedding = nn.Embedding(vocab_size, embedding_dim) # as context\n",
    "        \n",
    "        # 全连接层形状\n",
    "        self.linear = \n",
    "        # 输出层形状\n",
    "    def __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1a71f-ddb5-4e18-937c-339cebc5c0c0",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f226e94b-ca5a-43fb-8213-7df5eb5759ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "739b83f5-3a15-43f7-994d-036d2c448c20",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bfe199-2c2e-4679-a574-2ea2a0564d13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
